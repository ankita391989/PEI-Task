# 3b. Create an enriched table which has orders Information_Customer Name & Country_UnitTests

import pytest
from pyspark.sql import functions as F
from unittest import mock

# ------------------- Fixtures for Spark test data setup -------------------

@pytest.fixture
def sample_orders_df(spark):
    # Orders DataFrame with edge cases: nulls, negative, zero, and valid values
    data = [
        {"customer_id": "C1", "product_id": "P1", "order_id": 1, "order_date": "01/01/2022", "price": 100, "profit": 10.12, "quantity": 2},
        {"customer_id": "C2", "product_id": "P2", "order_id": 2, "order_date": "15/02/2022", "price": 200, "profit": 20.34, "quantity": 3},
        {"customer_id": "C3", "product_id": "P3", "order_id": 3, "order_date": None, "price": None, "profit": None, "quantity": None},
        {"customer_id": None, "product_id": "P1", "order_id": 4, "order_date": "01/03/2022", "price": 0, "profit": 0, "quantity": 0},
        {"customer_id": "C2", "product_id": "P2", "order_id": 5, "order_date": "01/04/2022", "price": -50, "profit": -5, "quantity": -1}
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def sample_customers_df(spark):
    # Customers DataFrame with valid and missing customers
    data = [
        {"customer_id": "C1", "customer_name": "Alice", "country": "US"},
        {"customer_id": "C2", "customer_name": "Bob", "country": "UK"},
        {"customer_id": "C4", "customer_name": "Eve", "country": "CA"}
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def customer_metrics(sample_orders_df):
    # Aggregation logic for customer metrics
    metrics = sample_orders_df.groupby('customer_id').agg(
        F.countDistinct('order_id').alias('total_orders'),
        F.sum('price').alias('total_sales'),
        F.sum('profit').alias('total_profit'),
        F.sum('quantity').alias('total_quantity'),
        F.min('order_date').alias('first_order_date'),
        F.max('order_date').alias('last_order_date')
    )
    metrics = metrics.withColumn('total_sales', F.round('total_sales', 2)) \
                     .withColumn('total_profit', F.round('total_profit', 2)) \
                     .withColumn('total_quantity', F.round('total_quantity', 2))
    return metrics

@pytest.fixture
def enriched_customers(sample_customers_df, customer_metrics):
    # Left join customers with metrics
    return sample_customers_df.join(customer_metrics, on='customer_id', how='left')

# ------------------- Parametrized and Edge Case Tests -------------------

@pytest.mark.parametrize("orders_data,expected_count", [
    ([{"customer_id": "C1", "product_id": "P1", "order_id": 1, "order_date": "01/01/2022", "price": 100, "profit": 10, "quantity": 2}], 1),
    ([], 0),
])
def test_customer_metrics_row_count(spark, orders_data, expected_count):
    """
    Test that customer_metrics row count matches number of unique customer_id in orders.
    """
    df = spark.createDataFrame(orders_data)
    metrics = df.groupby('customer_id').agg(F.countDistinct('order_id').alias('total_orders'))
    assert metrics.count() == expected_count

def test_enriched_customers_left_join(sample_customers_df, customer_metrics, enriched_customers):
    """
    Test that all customers appear in enriched_customers after left join, even if no orders.
    """
    customer_ids = set(r['customer_id'] for r in sample_customers_df.select('customer_id').collect())
    enriched_ids = set(r['customer_id'] for r in enriched_customers.select('customer_id').collect())
    assert customer_ids == enriched_ids

@pytest.mark.parametrize("col_name", ['total_sales', 'total_profit', 'total_quantity'])
def test_metrics_non_negative_or_null(customer_metrics, col_name):
    """
    Test that metrics columns are either non-negative or null (no negative values allowed).
    """
    min_value = customer_metrics.agg(F.min(col_name)).collect()[0][0]
    assert min_value is None or min_value >= 0

def test_metrics_rounding_precision(customer_metrics):
    """
    Test that numeric metrics are rounded to 2 decimals.
    """
    for col in ['total_sales', 'total_profit', 'total_quantity']:
        vals = customer_metrics.select(col).rdd.flatMap(lambda x: x).filter(lambda x: x is not None).collect()
        for v in vals:
            assert abs(v - round(v, 2)) < 1e-9

def test_first_order_date_before_last(customer_metrics):
    """
    Test that first_order_date is not after last_order_date for any customer.
    """
    for row in customer_metrics.select('first_order_date', 'last_order_date').collect():
        if row['first_order_date'] and row['last_order_date']:
            assert row['first_order_date'] <= row['last_order_date']

def test_null_metrics_for_customers_with_no_orders(enriched_customers):
    """
    Test that customers with no orders have null metrics.
    """
    left_only = enriched_customers.filter(F.col('total_orders').isNull())
    for row in left_only.collect():
        assert all(row[c] is None for c in ['total_sales', 'total_profit', 'total_quantity', 'first_order_date', 'last_order_date'])

def test_unique_customer_id_in_metrics(customer_metrics):
    """
    Test that customer_id is unique in customer_metrics.
    """
    total = customer_metrics.count()
    distinct = customer_metrics.select('customer_id').distinct().count()
    assert total == distinct

@pytest.mark.parametrize("col,expected_type", [
    ('customer_id', 'string'),
    ('total_orders', 'long'),
    ('total_sales', 'double'),
    ('total_profit', 'double'),
    ('total_quantity', 'double'),
    ('first_order_date', 'string'),
    ('last_order_date', 'string')
])
def test_customer_metrics_schema_types(customer_metrics, col, expected_type):
    """
    Test that customer_metrics columns have expected Spark types.
    """
    dtype = customer_metrics.schema[col].dataType.typeName()
    # Dates may be string if not parsed, so allow string/timestamp
    if col in ['first_order_date', 'last_order_date']:
        assert dtype in ['string', 'timestamp']
    else:
        assert dtype == expected_type

def test_enriched_customers_schema(enriched_customers, sample_customers_df, customer_metrics):
    """
    Test that enriched_customers contains all columns from both source DataFrames.
    """
    expected = set(sample_customers_df.columns) | set(customer_metrics.columns) - {'customer_id'}
    actual = set(enriched_customers.columns)
    assert expected.issubset(actual)

# ------------------- Mocking and Error Handling Tests -------------------

def test_groupby_agg_mock(monkeypatch):
    """
    Mock groupby/agg to ensure aggregation logic is called.
    """
    df = mock.Mock()
    df.groupby.return_value.agg.return_value = "metrics"
    result = df.groupby('customer_id').agg(F.countDistinct('order_id').alias('total_orders'))
    df.groupby.assert_called_once_with('customer_id')
    assert result == "metrics"

@pytest.mark.parametrize("missing_col", ['NonExistentColumn', 'AnotherMissing'])
def test_error_on_missing_column_in_join(sample_customers_df, customer_metrics, missing_col):
    """
    Test that joining on a missing column raises AnalysisException.
    """
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        sample_customers_df.join(customer_metrics, on=missing_col, how='left').count()

@pytest.mark.parametrize("agg_col", ['NonExistentColumn', 'AnotherMissing'])
def test_error_on_invalid_agg_column(sample_orders_df, agg_col):
    """
    Test that aggregating on a non-existent column raises AnalysisException.
    """
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        sample_orders_df.groupby('customer_id').agg(F.sum(agg_col)).count()

# ------------------- Date Parsing Logic Tests -------------------

@pytest.mark.parametrize("date_str,expected", [
    ("1/1/2022", "2022-01-01"),
    ("31/12/2021", "2021-12-31"),
    ("15/2/2022", "2022-02-15"),
    ("2022/01/01", None),  # Invalid format
    ("", None),            # Empty string
    (None, None)           # None value
])
def test_parse_date_logic(date_str, expected):
      """
    Test that dd/mm/yyyy date strings are parsed correctly, and invalid formats return None.
    """
    import pandas as pd
    def parse_date(date_str):
        try:
            return pd.to_datetime(date_str, format='%d/%m/%Y').strftime("%Y-%m-%d")
        except Exception:
            return None
    assert parse_date(date_str) == expected

# ------------------- Column Name Sanitization Logic -------------------

@pytest.mark.parametrize("input_cols,expected_cols", [
    (["customer id", "order date"], ["customer_id", "order_date"]),
    (["total sales", "profit"], ["total_sales", "profit"]),
    (["first_order_date", "last_order_date"], ["first_order_date", "last_order_date"])
])
def test_column_name_sanitization(input_cols, expected_cols):
    """
    Test that column name sanitization replaces spaces with underscores and preserves names.
    """
    import pandas as pd
    df = pd.DataFrame(columns=input_cols)
    sanitized = [col.replace(' ', '_') for col in input_cols]
    assert sanitized == expected_cols

# ------------------- Data Integrity After Transformations -------------------

def test_enriched_customers_row_count(sample_customers_df, enriched_customers):
    """
    Test that enriched_customers row count matches customers row count (left join).
    """
    assert enriched_customers.count() == sample_customers_df.count()