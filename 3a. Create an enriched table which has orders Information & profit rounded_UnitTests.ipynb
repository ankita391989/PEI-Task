import pytest

def test_column_renaming_success():
    # All columns should have underscores instead of spaces
    for df in [orders_df, products_df, customers_df]:
        for col_name in df.columns:
            assert " " not in col_name, f"Column '{col_name}' contains spaces after renaming."

def test_spark_dataframe_creation():
    # DataFrames should be Spark DataFrames
    from pyspark.sql import DataFrame
    assert isinstance(spark_orders_df, DataFrame)
    assert isinstance(spark_products_df, DataFrame)
    assert isinstance(spark_customers_df, DataFrame)

def test_dropna_removes_nulls():
    # No nulls in spark_orders_df and spark_customers_df
    assert spark_orders_df.filter(" OR ".join([f"{c} IS NULL" for c in spark_orders_df.columns])).count() == 0
    assert spark_customers_df.filter(" OR ".join([f"{c} IS NULL" for c in spark_customers_df.columns])).count() == 0

def test_customer_name_cleaning():
    # Customer_Name should not contain digits or special characters
    import re
    rows = spark_customers_df.select("Customer_Name").collect()
    for row in rows:
        assert re.match(r"^[a-zA-Z ]*$", row["Customer_Name"]), f"Customer_Name '{row['Customer_Name']}' contains invalid characters."

def test_enriched_orders_inner_join():
    # All Customer_IDs and Product_IDs in enriched_orders should exist in spark_customers_df and spark_products_df
    customer_ids = set(r["Customer_ID"] for r in spark_customers_df.select("Customer_ID").collect())
    product_ids = set(r["Product_ID"] for r in spark_products_df.select("Product_ID").collect())
    for row in enriched_orders.select("Customer_ID", "Product_ID").collect():
        assert row["Customer_ID"] in customer_ids, "Customer_ID in enriched_orders not found in spark_customers_df"
        assert row["Product_ID"] in product_ids, "Product_ID in enriched_orders not found in spark_products_df"

def test_enriched_orders_schema():
    # Check that enriched_orders has all expected columns
    expected_cols = {
        "Order_ID", "Product_ID", "Customer_ID", "Order_Date", "Ship_Date", "Price", "Profit", "Quantity",
        "Customer_Name", "Country", "Product_Name", "Category", "Sub-Category"
    }
    actual_cols = set(enriched_orders.columns)
    assert expected_cols.issubset(actual_cols), f"Missing columns in enriched_orders: {expected_cols - actual_cols}"

def test_enriched_orders_not_empty():
    # enriched_orders should not be empty if input data is not empty
    if spark_orders_df.count() > 0 and spark_customers_df.count() > 0 and spark_products_df.count() > 0:
        assert enriched_orders.count() > 0, "enriched_orders is empty despite non-empty input tables"

def test_error_on_missing_column_rename():
    # Renaming a non-existent column should raise a KeyError in pandas
    import pandas as pd
    df = pd.DataFrame({"A": [1], "B": [2]})
    with pytest.raises(KeyError):
        df.rename(columns={"C": "D"}, errors="raise")

def test_join_with_missing_keys():
    # If a Customer_ID or Product_ID is missing, those orders should not appear in enriched_orders
    import pandas as pd
    orders = pd.DataFrame({"Order_ID": [1], "Product_ID": ["P999"], "Customer_ID": ["C999"], "Order_Date": ["2020-01-01"], "Ship_Date": ["2020-01-02"], "Price": [10], "Profit": [2], "Quantity": [1]})
    products = pd.DataFrame({"Product_ID": ["P1"], "Product_Name": ["Test"], "Category": ["Cat"], "Sub-Category": ["Sub"]})
    customers = pd.DataFrame({"Customer_ID": ["C1"], "Customer_Name": ["Test"], "Country": ["X"]})
    sdf_orders = spark.createDataFrame(orders)
    sdf_products = spark.createDataFrame(products)
    sdf_customers = spark.createDataFrame(customers)
    result = sdf_orders.join(
        sdf_customers.select('Customer_ID', 'Customer_Name', 'Country'),
        on='Customer_ID', how='inner'
    ).join(
        sdf_products.select('Product_ID', 'Product_Name', 'Category', 'Sub-Category'),
        on='Product_ID', how='inner'
    )
    assert result.count() == 0, "Join did not filter out orders with missing keys"

def test_data_integrity_after_transforms():
    # Check that row counts are as expected after dropna and joins
    orders_count = spark_orders_df.count()
    customers_count = spark_customers_df.count()
    products_count = spark_products_df.count()
    # After inner join, count should not exceed any input
    assert enriched_orders.count() <= min(orders_count, customers_count, products_count)

def test_save_and_load_tables(tmp_path):
    # Save and reload a table, check data integrity
    test_table = "test_enriched_orders"
    enriched_orders.write.mode("overwrite").saveAsTable(test_table)
    loaded = spark.table(test_table)
    assert loaded.count() == enriched_orders.count()
    assert set(loaded.columns) == set(enriched_orders.columns)
    spark.sql(f"DROP TABLE IF EXISTS {test_table}")

def test_dropna_orders():
    # Test that rows with missing values are dropped in orders DataFrame
    total_rows = spark.createDataFrame(orders_df).count()
    cleaned_rows = spark_orders_df.count()
    assert cleaned_rows <= total_rows, "Dropna failed to clean missing rows in Orders DataFrame."

def test_dropna_customers():
    # Test that rows with missing values are dropped in customers DataFrame
    total_rows = spark.createDataFrame(customers_df).count()
    cleaned_rows = spark_customers_df.count()
    assert cleaned_rows <= total_rows, "Dropna failed to clean missing rows in Customers DataFrame."