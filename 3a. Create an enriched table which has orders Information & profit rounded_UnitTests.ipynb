# 3a. Create an enriched table which has orders Information & profit rounded_UnitTests
import pytest
from unittest import mock

# ------------------- Fixtures for Spark test data setup -------------------

@pytest.fixture
def sample_orders_pd():
    # Orders DataFrame with valid and invalid data for join and dropna tests
    import pandas as pd
    data = [
        {"order_id": 1, "product_id": "P1", "customer_id": "C1", "order_date": "01/01/2022", "ship_date": "02/01/2022", "price": 10, "profit": 2.123, "quantity": 1},
        {"order_id": 2, "product_id": "P2", "customer_id": "C2", "order_date": "15/02/2022", "ship_date": "16/02/2022", "price": 20, "profit": 5.456, "quantity": 2},
        {"order_id": 3, "product_id": None, "customer_id": "C1", "order_date": None, "ship_date": "17/02/2022", "price": 30, "profit": None, "quantity": 3}
    ]
    return pd.DataFrame(data)

@pytest.fixture
def sample_products_pd():
    import pandas as pd
    data = [
        {"product_id": "P1", "product_name": "Prod1", "category": "Cat1", "sub_category": "Sub1"},
        {"product_id": "P2", "product_name": "Prod2", "category": "Cat2", "sub_category": "Sub2"}
    ]
    return pd.DataFrame(data)

@pytest.fixture
def sample_customers_pd():
    import pandas as pd
    data = [
        {"customer_id": "C1", "customer_name": "Alice123!", "country": "US"},
        {"customer_id": "C2", "customer_name": "Bob@#", "country": "UK"}
    ]
    return pd.DataFrame(data)

@pytest.fixture
def spark_orders_df(sample_orders_pd):
    # Spark DataFrame for orders, with some nulls
    return spark.createDataFrame(sample_orders_pd)

@pytest.fixture
def spark_products_df(sample_products_pd):
    return spark.createDataFrame(sample_products_pd)

@pytest.fixture
def spark_customers_df(sample_customers_pd):
    return spark.createDataFrame(sample_customers_pd)

@pytest.fixture
def enriched_orders(spark_orders_df, spark_products_df, spark_customers_df):
    # Simulate enrichment logic for test purposes
    from pyspark.sql.functions import trim, regexp_replace, col, round as spark_round
    # Clean customer_name
    customers_clean = spark_customers_df.withColumn(
        "customer_name",
        trim(
            regexp_replace(
                regexp_replace("customer_name", "[^a-zA-Z ]", ""),
                " +",
                " "
            )
        )
    )
    # Drop nulls
    orders_clean = spark_orders_df.dropna()
    customers_clean = customers_clean.dropna()
    # Round profit
    orders_clean = orders_clean.withColumn("profit", spark_round("profit", 2))
    # Join
    enriched = orders_clean.join(
        customers_clean.select('customer_id', 'customer_name', 'country'),
        on='customer_id', how='inner'
    ).join(
        spark_products_df.select('product_id', 'product_name', 'category', 'sub_category'),
        on='product_id', how='inner'
    )
    return enriched

# ------------------- Parametrized tests for column name sanitization -------------------

@pytest.mark.parametrize("input_cols,expected_cols", [
    (["order id", "product id"], ["order_id", "product_id"]),
    (["customer name", "country"], ["customer_name", "country"]),
    (["profit", "quantity"], ["profit", "quantity"])
])
def test_column_renaming(input_cols, expected_cols):
    """
    Test that column renaming logic replaces spaces with underscores and preserves names.
    """
    import pandas as pd
    df = pd.DataFrame(columns=input_cols)
    df = df.rename(columns=lambda x: x.replace(" ", "_"))
    assert [c.lower() for c in df.columns] == expected_cols

# ------------------- Test: Spark DataFrame creation -------------------

def test_spark_dataframe_type(spark_orders_df, spark_products_df, spark_customers_df):
    """
    Ensure Spark DataFrames are created from pandas DataFrames.
    """
    from pyspark.sql import DataFrame
    # Assert that each DataFrame is an instance of Spark's DataFrame class
    assert isinstance(spark_orders_df, DataFrame)
    assert isinstance(spark_products_df, DataFrame)
    assert isinstance(spark_customers_df, DataFrame)

# ------------------- Test to ensure that all null values are removed from spark_orders_df and spark_customers_df-------------------

@pytest.mark.parametrize("df_fixture", ["spark_orders_df", "spark_customers_df"])
def test_dropna_removes_nulls(request, df_fixture):
    """
    Ensure dropna removes all rows with nulls in Spark DataFrames.
    """
    df = request.getfixturevalue(df_fixture)
    null_count = df.filter(" OR ".join([f"{c} IS NULL" for c in df.columns])).count()
    assert null_count == 0

# ------------------- Test:To ensure that the customer_name column in spark_customers_df contains only valid characters (letters and spaces) -------------------

@pytest.mark.parametrize("raw_name,expected_clean", [
    ("Alice123!", "Alice"),
    ("Bob@#", "Bob"),
    ("John Doe", "John Doe"),
    ("Mary_Ann", "MaryAnn")
])
def test_customer_name_cleaning(raw_name, expected_clean):
    """
    Ensure customer_name cleaning removes non-letter characters and trims spaces.
    """
    from pyspark.sql import Row
    df = spark.createDataFrame([Row(customer_name=raw_name)])
    from pyspark.sql.functions import trim, regexp_replace
    cleaned = df.withColumn(
        "customer_name",
        trim(
            regexp_replace(
                regexp_replace("customer_name", "[^a-zA-Z ]", ""),
                " +",
                " "
            )
        )
    ).collect()[0]["customer_name"]
    assert cleaned == expected_clean

# ------------------- Test: Enriched orders join logic -------------------

def test_enriched_orders_inner_join(enriched_orders, spark_customers_df, spark_products_df):
    """
    Ensure all customer_id and product_id in enriched_orders exist in source DataFrames.
    """
    customer_ids = set(r["customer_id"] for r in spark_customers_df.select("customer_id").collect())
    product_ids = set(r["product_id"] for r in spark_products_df.select("product_id").collect())
    # Check each row in enriched_orders for valid customer_id and product_id
    for row in enriched_orders.select("customer_id", "product_id").collect():
        assert row["customer_id"] in customer_ids
        assert row["product_id"] in product_ids

# ------------------- Test: Enriched orders schema -------------------

def test_enriched_orders_schema(enriched_orders):
    """
    Ensure enriched_orders contains all expected columns.
    """
    expected_cols = {
        "order_id", "product_id", "customer_id", "order_date", "ship_date", "price", "profit", "quantity",
        "customer_name", "country", "product_name", "category", "sub_category"
    }
    # Get the actual columns from enriched_orders
    actual_cols = set(enriched_orders.columns)
    assert expected_cols.issubset(actual_cols)

# ------------------- Test: Enriched orders not empty if inputs not empty -------------------

def test_enriched_orders_not_empty(enriched_orders, spark_orders_df, spark_customers_df, spark_products_df):
    """
    Ensure enriched_orders is not empty if all input DataFrames are non-empty.
    """
    if spark_orders_df.count() > 0 and spark_customers_df.count() > 0 and spark_products_df.count() > 0:
        assert enriched_orders.count() > 0

# ------------------- Test: Date parsing logic -------------------

@pytest.mark.parametrize("date_str,expected", [
    ("1/1/2022", "2022-01-01"),
    ("31/12/2021", "2021-12-31"),
    ("15/2/2022", "2022-02-15"),
    ("2022/01/01", None)  # Invalid format
])
def test_parse_date_logic(date_str, expected):
    """
    Ensure dd/mm/yyyy date strings are parsed correctly, and invalid formats return NaT.
    """
    import pandas as pd
    def parse_date(date_str):
        try:
            return pd.to_datetime(date_str, format='%d/%m/%Y').strftime("%Y-%m-%d")
        except Exception:
            return None
    assert parse_date(date_str) == expected

# ------------------- Test: Data integrity after transformations -------------------

def test_data_integrity_after_transforms(enriched_orders, spark_orders_df, spark_customers_df, spark_products_df):
    """
    Ensure enriched_orders row count does not exceed the smallest input DataFrame after joins.
    """
    min_count = min(spark_orders_df.count(), spark_customers_df.count(), spark_products_df.count())
    assert enriched_orders.count() <= min_count

# ------------------- Test: Save and load table roundtrip -------------------

def test_save_and_load_tables(enriched_orders):
    """
    Ensure saving and loading enriched_orders table preserves row count and schema.
    """
    test_table = "test_enriched_orders"
    enriched_orders.write.mode("overwrite").saveAsTable(test_table)
    loaded = spark.table(test_table)
    assert loaded.count() == enriched_orders.count()
    assert set(loaded.columns) == set(enriched_orders.columns)
    spark.sql(f"DROP TABLE IF EXISTS {test_table}")

# ------------------- Test: Mocking Spark DataFrame dropna -------------------

def test_dropna_mock(monkeypatch):
    """
    Mock Spark DataFrame.dropna to isolate logic and ensure dropna is called.
    """
    from pyspark.sql import DataFrame
    df = mock.Mock(spec=DataFrame)
    df.dropna.return_value = "cleaned"
    result = df.dropna()
    df.dropna.assert_called_once()
    assert result == "cleaned"

# ------------------- Test: Error handling for missing column rename -------------------

def test_error_on_missing_column_rename():
    """
    Ensure renaming a non-existent column in pandas raises KeyError.
    """
    import pandas as pd
    # Create a sample DataFrame
    df = pd.DataFrame({"A": [1], "B": [2]})
    # Attempt to rename a non-existent column and expect a KeyError
    with pytest.raises(KeyError):
        df.rename(columns={"C": "D"}, errors="raise")

# ------------------- Test: Join filters out orders with missing keys -------------------

def test_join_with_missing_keys():
    """
    Ensure join filters out orders with missing product_id/customer_id.
    """
    import pandas as pd
    # Create sample DataFrames with missing keys
    orders = pd.DataFrame({"order_id": [1], "product_id": ["P999"], "customer_id": ["C999"], "order_date": ["2020-01-01"], "ship_date": ["2020-01-02"], "price": [10], "profit": [2], "quantity": [1]})
    products = pd.DataFrame({"product_id": ["P1"], "product_name": ["Test"], "category": ["Cat"], "sub_category": ["Sub"]})
    customers = pd.DataFrame({"customer_id": ["C1"], "customer_name": ["Test"], "country": ["X"]})
    # Convert to Spark DataFrames
    sdf_orders = spark.createDataFrame(orders)
    sdf_products = spark.createDataFrame(products)
    sdf_customers = spark.createDataFrame(customers)
    # Perform inner joins
    result = sdf_orders.join(
        sdf_customers.select('customer_id', 'customer_name', 'country'),
        on='customer_id', how='inner'
    ).join(
        sdf_products.select('product_id', 'product_name', 'category', 'sub_category'),
        on='product_id', how='inner'
    )
    assert result.count() == 0

# ------------------- Test: Dropna reduces row count if nulls present -------------------

@pytest.mark.parametrize("df_fixture", ["sample_orders_pd", "sample_customers_pd"])
def test_dropna_reduces_rows(df_fixture, request):
    """
    Ensure dropna reduces row count if nulls are present in pandas DataFrame.
    """
    df = request.getfixturevalue(df_fixture)
    total_rows = len(df)
    cleaned_rows = len(df.dropna())
    assert cleaned_rows <= total_rows