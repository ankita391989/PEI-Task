#4abcd.Create an aggregate table that shows profit by_UnitTests

import pytest
from pyspark.sql import functions as F
from unittest import mock

# ------------------- Fixtures for Spark test data setup -------------------

@pytest.fixture
def sample_enriched_orders(spark):
    # Setup: DataFrame with edge cases for profit aggregation
    data = [
        {"order_id": 1, "product_id": "P1", "customer_id": "C1", "order_date": "01/01/2022", "year": 2022, "category": "Cat1", "sub_category": "Sub1", "customer_name": "Alice", "profit": 10.12},
        {"order_id": 2, "product_id": "P2", "customer_id": "C2", "order_date": "15/02/2022", "year": 2022, "category": "Cat2", "sub_category": "Sub2", "customer_name": "Bob", "profit": 20.34},
        {"order_id": 3, "product_id": "P3", "customer_id": "C3", "order_date": "01/01/2023", "year": 2023, "category": "Cat1", "sub_category": "Sub1", "customer_name": "Alice", "profit": None},
        {"order_id": 4, "product_id": "P1", "customer_id": "C1", "order_date": "01/03/2022", "year": 2022, "category": "Cat1", "sub_category": "Sub1", "customer_name": None, "profit": 0},
        {"order_id": 5, "product_id": "P2", "customer_id": "C2", "order_date": "01/04/2022", "year": None, "category": None, "sub_category": None, "customer_name": "Bob", "profit": -5}
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def profit_aggregates(sample_enriched_orders):
    # Returns all aggregate DataFrames for testing
    profit_by_year = sample_enriched_orders.groupby('year').agg(F.round(F.sum('profit'), 2).alias('total_profit'))
    profit_by_category = sample_enriched_orders.groupby('category').agg(F.round(F.sum('profit'), 2).alias('total_profit'))
    profit_by_sub_category = sample_enriched_orders.groupby('sub_category').agg(F.round(F.sum('profit'), 2).alias('total_profit'))
    profit_by_cust = sample_enriched_orders.groupby('customer_name').agg(F.round(F.sum('profit'), 2).alias('total_profit'))
    return {
        'profit_by_year': profit_by_year,
        'profit_by_category': profit_by_category,
        'profit_by_sub_category': profit_by_sub_category,
        'profit_by_cust': profit_by_cust
    }

# ------------------- Parametrized and Edge Case Tests -------------------

@pytest.mark.parametrize("agg_name,group_col,expected_cols", [
    ('profit_by_year', 'year', {'year', 'total_profit'}),
    ('profit_by_category', 'category', {'category', 'total_profit'}),
    ('profit_by_sub_category', 'sub_category', {'sub_category', 'total_profit'}),
    ('profit_by_cust', 'customer_name', {'customer_name', 'total_profit'})
])
def test_aggregate_schema_and_group_keys(profit_aggregates, sample_enriched_orders, agg_name, group_col, expected_cols):
    """
    Ensure each aggregate DataFrame has correct schema and group keys match those in source.
    """
    df = profit_aggregates[agg_name]
    # Check schema
    assert set(df.columns) == expected_cols
    # Check group keys are subset of source keys (including nulls)
    src_keys = set(r[group_col] for r in sample_enriched_orders.select(group_col).distinct().collect())
    agg_keys = set(r[group_col] for r in df.select(group_col).distinct().collect())
    assert agg_keys.issubset(src_keys)

@pytest.mark.parametrize("agg_name", ['profit_by_year', 'profit_by_category', 'profit_by_sub_category', 'profit_by_cust'])
def test_aggregate_not_empty_if_orders(profit_aggregates, sample_enriched_orders, agg_name):
    """
    Ensure aggregate DataFrames are not empty if source is not empty.
    """
    df = profit_aggregates[agg_name]
    if sample_enriched_orders.count() > 0:
        assert df.count() > 0

@pytest.mark.parametrize("agg_name", ['profit_by_year', 'profit_by_category', 'profit_by_sub_category', 'profit_by_cust'])
def test_total_profit_rounded_to_2_decimals(profit_aggregates, agg_name):
    """
    Ensure total_profit is rounded to 2 decimals in all aggregates.
    """
    df = profit_aggregates[agg_name]
    profits = df.select('total_profit').rdd.flatMap(lambda x: x).filter(lambda x: x is not None).collect()
    for v in profits:
        assert abs(v - round(v, 2)) < 1e-9

@pytest.mark.parametrize("agg_name,group_col", [
    ('profit_by_year', 'year'),
    ('profit_by_category', 'category'),
    ('profit_by_sub_category', 'sub_category'),
    ('profit_by_cust', 'customer_name')
])
def test_no_duplicate_group_keys_in_aggregates(profit_aggregates, agg_name, group_col):
    """
    Ensure group keys are unique in each aggregate DataFrame.
    """
    df = profit_aggregates[agg_name]
    total = df.count()
    distinct = df.select(group_col).distinct().count()
    assert total == distinct

def test_total_profit_consistency_across_aggregates(profit_aggregates):
    """
    Ensure total profit is consistent across year, category, and customer aggregates.
    """
    sums = []
    for agg_name in ['profit_by_year', 'profit_by_category', 'profit_by_cust']:
        df = profit_aggregates[agg_name]
        val = df.agg(F.sum('total_profit')).collect()[0][0]
        sums.append(val)
    # Allow for None if all profits are null
    if all(v is not None for v in sums):
        assert max(sums) - min(sums) < 1e-6

@pytest.mark.parametrize("agg_name,group_col", [
    ('profit_by_year', 'year'),
    ('profit_by_category', 'category'),
    ('profit_by_sub_category', 'sub_category'),
    ('profit_by_cust', 'customer_name')
])
def test_null_group_keys_handling(profit_aggregates, sample_enriched_orders, agg_name, group_col):
    """
    Ensure null group keys are handled and present in aggregates if present in source.
    """
    src_nulls = sample_enriched_orders.filter(F.col(group_col).isNull()).count()
    df = profit_aggregates[agg_name]
    agg_nulls = df.filter(F.col(group_col).isNull()).count()
    if src_nulls > 0:
        assert agg_nulls > 0

def test_empty_source_results_in_empty_aggregates(spark):
    """
    Ensure empty source DataFrame results in empty aggregates.
    """
    empty_df = spark.createDataFrame([], schema="order_id INT, product_id STRING, customer_id STRING, order_date STRING, year INT, category STRING, sub_category STRING, customer_name STRING, profit DOUBLE")
    by_year = empty_df.groupby('year').agg(F.sum('profit').alias('total_profit'))
    assert by_year.count() == 0

# ------------------- Error Handling & Mocking -------------------

@pytest.mark.parametrize("missing_col", ['non_existent_column', 'another_missing'])
def test_error_on_missing_column_in_groupby(sample_enriched_orders, missing_col):
    """
    Ensure grouping by a missing column raises AnalysisException.
    """
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        sample_enriched_orders.groupby(missing_col).agg(F.sum('profit')).count()

@pytest.mark.parametrize("agg_col", ['non_existent_column', 'another_missing'])
def test_error_on_invalid_agg_column(sample_enriched_orders, agg_col):
    """
    Ensure aggregating on a missing column raises AnalysisException.
    """
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        sample_enriched_orders.groupby('year').agg(F.sum(agg_col)).count()

def test_groupby_agg_mock(monkeypatch):
    """
    Mock groupby/agg to ensure aggregation logic is called and isolated from Spark internals.
    """
    df = mock.Mock()
    df.groupby.return_value.agg.return_value = "aggregated"
    result = df.groupby('year').agg(F.sum('profit'))
    df.groupby.assert_called_once_with('year')
    assert result == "aggregated"

# ------------------- Date Parsing Logic -------------------

@pytest.mark.parametrize("date_str,expected", [
    ("01/01/2022", "2022-01-01"),
    ("31/12/2021", "2021-12-31"),
    ("15/02/2022", "2022-02-15"),
    ("2022/01/01", None),  # Invalid format
    ("", None),            # Empty string
    (None, None)           # None value
])
def test_parse_date_logic(date_str, expected):
    """
    Ensure dd/mm/yyyy date strings are parsed correctly, and invalid formats return None.
    """
    import pandas as pd
    def parse_date(date_str):
        try:
            return pd.to_datetime(date_str, format='%d/%m/%Y').strftime("%Y-%m-%d")
        except Exception:
            return None
    assert parse_date(date_str) == expected

# ------------------- Column Name Sanitization Logic -------------------

@pytest.mark.parametrize("input_cols,expected_cols", [
    (["order id", "product id"], ["order_id", "product_id"]),
    (["customer name", "category"], ["customer_name", "category"]),
    (["profit", "quantity"], ["profit", "quantity"])
])
def test_column_name_sanitization(input_cols, expected_cols):
    """
    Ensure column name sanitization replaces spaces with underscores and preserves names.
    """
    import pandas as pd
    df = pd.DataFrame(columns=input_cols)
    sanitized = [col.replace(' ', '_') for col in df.columns]
    assert sanitized == expected_cols