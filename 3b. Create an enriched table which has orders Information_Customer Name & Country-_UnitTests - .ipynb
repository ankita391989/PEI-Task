import pytest

# success test cases

def test_customer_metrics_not_empty():
    assert customer_metrics.count() >= 0, "customer_metrics DataFrame should exist."

def test_enriched_customers_not_empty():
    assert enriched_customers.count() >= 0, "enriched_customers DataFrame should exist."

def test_customer_metrics_columns():
    expected_columns = {
        'Customer_ID', 'Total_Orders', 'Total_Sales', 'Total_Profit',
        'Total_Quantity', 'First_Order_Date', 'Last_Order_Date'
    }
    actual_columns = set(customer_metrics.columns)
    assert expected_columns.issubset(actual_columns), "Customer metrics columns missing."

def test_enriched_customers_join():
    # All Customer_IDs in spark_customers_df should be present in enriched_customers
    customer_ids = set(row['Customer_ID'] for row in spark_customers_df.select('Customer_ID').collect())
    enriched_ids = set(row['Customer_ID'] for row in enriched_customers.select('Customer_ID').collect())
    assert customer_ids.issubset(enriched_ids), "Left join failed: missing Customer_IDs in enriched_customers."

def test_total_orders_non_negative():
    # Total_Orders should be non-negative
    min_orders = customer_metrics.agg(F.min('Total_Orders')).collect()[0][0]
    assert min_orders is None or min_orders >= 0, "Total_Orders contains negative values."

def test_total_sales_profit_quantity_non_negative():
     # Total_Sales, Total_Profit, and Total_Quantity should be non-negative or null
    for col_name in ['Total_Sales', 'Total_Profit', 'Total_Quantity']:
        min_value = customer_metrics.agg(F.min(col_name)).collect()[0][0]
        assert (min_value is None) or (min_value >= 0), f"{col_name} contains negative values."

def test_first_last_order_date_order():
    # First_Order_Date should be less than or equal to Last_Order_Date
    rows = customer_metrics.select('First_Order_Date', 'Last_Order_Date').collect()
    for row in rows:
        if row['First_Order_Date'] and row['Last_Order_Date']:
            assert row['First_Order_Date'] <= row['Last_Order_Date'], "First_Order_Date is after Last_Order_Date."

#edge cases and error handling

def test_enriched_customers_all_customers_present():
    # All customers from spark_customers_df should be present in enriched_customers
    base_count = spark_customers_df.count()
    enriched_count = enriched_customers.select('Customer_ID').distinct().count()
    assert base_count == enriched_count, "Not all customers are present in enriched_customers."

def test_enriched_customers_null_metrics_for_no_orders():
    # Customers with no orders should have null metrics
    left_only = enriched_customers.filter(F.col('Total_Orders').isNull())
    if left_only.count() > 0:
        for row in left_only.collect():
            assert row['Total_Sales'] is None
            assert row['Total_Profit'] is None
            assert row['Total_Quantity'] is None
            assert row['First_Order_Date'] is None
            assert row['Last_Order_Date'] is None

def test_customer_metrics_unique_customer_id():
    # Customer_ID should be unique in customer_metrics
    total = customer_metrics.count()
    distinct = customer_metrics.select('Customer_ID').distinct().count()
    assert total == distinct, "Customer_ID is not unique in customer_metrics."

def test_customer_metrics_data_types():
    # Check data types of columns
    schema = dict((f.name, f.dataType.typeName()) for f in customer_metrics.schema.fields)
    assert schema['Customer_ID'] == 'string'
    assert schema['Total_Orders'] == 'long'
    assert schema['Total_Sales'] in ['double', 'decimal']
    assert schema['Total_Profit'] in ['double', 'decimal']
    assert schema['Total_Quantity'] in ['double', 'decimal', 'long']
    assert schema['First_Order_Date'] == 'timestamp'
    assert schema['Last_Order_Date'] == 'timestamp'

def test_customer_metrics_empty_orders():
    # Edge: If spark_orders_df is empty, customer_metrics should be empty
    empty_orders = spark_orders_df.limit(0)
    empty_metrics = empty_orders.groupby('Customer_ID').agg(
        F.countDistinct('Order_ID').alias('Total_Orders')
    )
    assert empty_metrics.count() == 0, "customer_metrics should be empty if no orders."

def test_enriched_customers_empty_customers():
    # Edge: If spark_customers_df is empty, enriched_customers should be empty
    empty_customers = spark_customers_df.limit(0)
    joined = empty_customers.join(customer_metrics, on='Customer_ID', how='left')
    assert joined.count() == 0, "enriched_customers should be empty if no customers."

def test_customer_metrics_null_customer_id():
    # Edge: If there are null Customer_IDs in orders, they should not appear in customer_metrics
    with_null = spark_orders_df.withColumn('Customer_ID', F.lit(None))
    metrics = with_null.groupby('Customer_ID').agg(F.count('*').alias('cnt'))
    for row in metrics.collect():
        assert row['Customer_ID'] is None, "Non-null Customer_ID found when all should be null."

def test_enriched_customers_schema_integrity():
    # All expected columns should be present in enriched_customers
    expected = set(spark_customers_df.columns) | set(customer_metrics.columns) - {'Customer_ID'}
    actual = set(enriched_customers.columns)
    assert expected.issubset(actual), "enriched_customers missing expected columns."

def test_customer_metrics_rounding():
    # Total_Sales, Total_Profit, Total_Quantity should be rounded to 2 decimals if not null
    for col in ['Total_Sales', 'Total_Profit', 'Total_Quantity']:
        vals = customer_metrics.select(col).rdd.flatMap(lambda x: x).filter(lambda x: x is not None)
        for v in vals.collect():
            assert round(v, 2) == v, f"{col} is not rounded to 2 decimals."

def test_error_on_missing_column_in_join():
    # Error handling: joining on missing column should raise AnalysisException
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        spark_customers_df.join(customer_metrics, on='NonExistentColumn', how='left').count()

def test_error_on_invalid_agg_column():
    # Error handling: aggregating on non-existent column should raise AnalysisException
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        spark_orders_df.groupby('Customer_ID').agg(F.sum('NonExistentColumn')).count()