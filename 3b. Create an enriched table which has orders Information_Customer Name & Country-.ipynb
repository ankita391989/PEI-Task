from pyspark.sql import functions as F

# Aggregate customer metrics - To get order information
customer_metrics = spark_orders_df.groupby('Customer_ID').agg(
    F.countDistinct('Order_ID').alias('Total_Orders'),
    F.sum('Price').alias('Total_Sales'),
    F.sum('Profit').alias('Total_Profit'),
    F.sum('Quantity').alias('Total_Quantity'),
    F.min('Order_Date').alias('First_Order_Date'),
    F.max('Order_Date').alias('Last_Order_Date')
)

# Round the numeric columns
customer_metrics = customer_metrics.withColumn('Total_Sales', F.round('Total_Sales', 2)) \
                                   .withColumn('Total_Profit', F.round('Total_Profit', 2)) \
                                   .withColumn('Total_Quantity', F.round('Total_Quantity', 2))

# Join with customer details, Left join with Customer and Customer Metrics  
enriched_customers = spark_customers_df.join(
    customer_metrics,
    on='Customer_ID',
    how='left'
)

display(enriched_customers)