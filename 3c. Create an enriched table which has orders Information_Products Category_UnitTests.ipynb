#3c. Create an enriched table which has orders Information_Products Category_UnitTests
import pytest
from pyspark.sql import functions as F
from unittest import mock

# ------------------- Fixtures for Spark test data setup -------------------

@pytest.fixture
def sample_orders_df(spark):
    # Orders DataFrame with edge cases: nulls, negative, zero, and valid values
    data = [
        {"order_id": 1, "product_id": "p1", "customer_id": "c1", "order_date": "01/01/2022", "price": 100, "profit": 10.12, "quantity": 2},
        {"order_id": 2, "product_id": "p2", "customer_id": "c2", "order_date": "15/02/2022", "price": 200, "profit": 20.34, "quantity": 3},
        {"order_id": 3, "product_id": "p3", "customer_id": "c3", "order_date": None, "price": None, "profit": None, "quantity": None},
        {"order_id": 4, "product_id": "p1", "customer_id": None, "order_date": "01/03/2022", "price": 0, "profit": 0, "quantity": 0},
        {"order_id": 5, "product_id": "p2", "customer_id": "c2", "order_date": "01/04/2022", "price": -50, "profit": -5, "quantity": -1}
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def sample_products_df(spark):
    # Products DataFrame with valid and missing products
    data = [
        {"product_id": "p1", "product_name": "prod1", "category": "cat1", "sub_category": "sub1"},
        {"product_id": "p2", "product_name": "prod2", "category": "cat2", "sub_category": "sub2"},
        {"product_id": "p4", "product_name": "prod4", "category": "cat4", "sub_category": "sub4"}
    ]
    return spark.createDataFrame(data)

@pytest.fixture
def product_metrics(sample_orders_df):
    # Aggregation logic for product metrics
    metrics = sample_orders_df.groupby('product_id').agg(
        F.countDistinct('order_id').alias('total_orders'),
        F.sum('price').alias('total_sales'),
        F.sum('profit').alias('total_profit'),
        F.sum('quantity').alias('total_quantity_sold'),
        F.countDistinct('customer_id').alias('unique_customers')
    )
    metrics = metrics.withColumn('total_sales', F.round('total_sales', 2)) \
                     .withColumn('total_profit', F.round('total_profit', 2))
    return metrics

@pytest.fixture
def enriched_products(sample_products_df, product_metrics):
    # Left join products with metrics
    return sample_products_df.join(product_metrics, on='product_id', how='left')

# ------------------- Parametrized and Edge Case Tests -------------------

@pytest.mark.parametrize("orders_data,expected_count", [
    ([{"order_id": 1, "product_id": "p1", "customer_id": "c1", "order_date": "01/01/2022", "price": 100, "profit": 10, "quantity": 2}], 1),
    ([], 0),
])
def test_product_metrics_row_count(spark, orders_data, expected_count):
    """
    Test that product_metrics row count matches number of unique product_id in orders.
    """
    df = spark.createDataFrame(orders_data)
    metrics = df.groupby('product_id').agg(F.countDistinct('order_id').alias('total_orders'))
    assert metrics.count() == expected_count

def test_enriched_products_left_join(sample_products_df, product_metrics, enriched_products):
    """
    Test that all products appear in enriched_products after left join, even if no orders.
    """
    product_ids = set(r['product_id'] for r in sample_products_df.select('product_id').collect())
    enriched_ids = set(r['product_id'] for r in enriched_products.select('product_id').collect())
    assert product_ids == enriched_ids

@pytest.mark.parametrize("col_name", ['total_orders', 'total_sales', 'total_profit', 'total_quantity_sold', 'unique_customers'])
def test_metrics_non_negative_or_null(product_metrics, col_name):
    """
    Test that metrics columns are either non-negative or null (no negative values allowed).
    """
    min_value = product_metrics.agg(F.min(col_name)).collect()[0][0]
    assert min_value is None or min_value >= 0

def test_metrics_rounding_precision(product_metrics):
    """
    Test that numeric metrics are rounded to 2 decimals.
    """
    for col in ['total_sales', 'total_profit']:
        vals = product_metrics.select(col).rdd.flatMap(lambda x: x).filter(lambda x: x is not None).collect()
        for v in vals:
            assert abs(v - round(v, 2)) < 1e-9

def test_null_metrics_for_products_with_no_orders(enriched_products):
    """
    Test that products with no orders have null metrics.
    """
    left_only = enriched_products.filter(F.col('total_orders').isNull())
    for row in left_only.collect():
        assert all(row[c] is None for c in ['total_sales', 'total_profit', 'total_quantity_sold', 'unique_customers'])

def test_unique_product_id_in_metrics(product_metrics):
    """
    Test that product_id is unique in product_metrics.
    """
    total = product_metrics.count()
    distinct = product_metrics.select('product_id').distinct().count()
    assert total == distinct

@pytest.mark.parametrize("col,expected_type", [
    ('product_id', 'string'),
    ('total_orders', 'long'),
    ('total_sales', 'double'),
    ('total_profit', 'double'),
    ('total_quantity_sold', 'double'),
    ('unique_customers', 'long')
])
def test_product_metrics_schema_types(product_metrics, col, expected_type):
    """
    Test that product_metrics columns have expected Spark types.
    """
    dtype = product_metrics.schema[col].dataType.typeName()
    assert dtype == expected_type

def test_enriched_products_schema(enriched_products, sample_products_df, product_metrics):
    """
    Test that enriched_products contains all columns from both source DataFrames.
    """
    expected = set(sample_products_df.columns) | set(product_metrics.columns) - {'product_id'}
    actual = set(enriched_products.columns)
    assert expected.issubset(actual)

def test_enriched_products_row_count(sample_products_df, enriched_products):
    """
    Test that enriched_products row count matches products row count (left join).
    """
    assert enriched_products.count() == sample_products_df.count()

# ------------------- Mocking and Error Handling Tests -------------------

def test_groupby_agg_mock(monkeypatch):
    """
    Mock groupby/agg to ensure aggregation logic is called.
    """
    df = mock.Mock()
    df.groupby.return_value.agg.return_value = "metrics"
    result = df.groupby('product_id').agg(F.countDistinct('order_id').alias('total_orders'))
    df.groupby.assert_called_once_with('product_id')
    assert result == "metrics"

@pytest.mark.parametrize("missing_col", ['nonexistentcolumn', 'anothermissing'])
def test_error_on_missing_column_in_join(sample_products_df, product_metrics, missing_col):
    """
    Test that joining on a missing column raises AnalysisException.
    """
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        sample_products_df.join(product_metrics, on=missing_col, how='left').count()

@pytest.mark.parametrize("agg_col", ['nonexistentcolumn', 'anothermissing'])
def test_error_on_invalid_agg_column(sample_orders_df, agg_col):
    """
    Test that aggregating on a non-existent column raises AnalysisException.
    """
    import pyspark
    with pytest.raises(pyspark.sql.utils.AnalysisException):
        sample_orders_df.groupby('product_id').agg(F.sum(agg_col)).count()

# ------------------- Column Name Sanitization Logic -------------------

@pytest.mark.parametrize("input_cols,expected_cols", [
    (["product id", "total sales"], ["product_id", "total_sales"]),
    (["unique customers", "profit"], ["unique_customers", "profit"]),
    (["total_orders", "total_profit"], ["total_orders", "total_profit"])
])
def test_column_name_sanitization(input_cols, expected_cols):
    """
    Test that column name sanitization replaces spaces with underscores and preserves names.
    """
    import pandas as pd
    df = pd.DataFrame(columns=input_cols)
    sanitized = [col.replace(' ', '_') for col in df.columns]
    assert sanitized == expected_cols

# ------------------- Date Parsing Logic Tests -------------------

@pytest.mark.parametrize("date_str,expected", [
    ("01/01/2022", "2022-01-01"),
    ("31/12/2021", "2021-12-31"),
    ("15/02/2022", "2022-02-15"),
    ("2022/01/01", None),  # Invalid format
    ("", None),            # Empty string
    (None, None)           # None value
])
def test_parse_date_logic(date_str, expected):
    """
    Test that dd/mm/yyyy date strings are parsed correctly, and invalid formats return None.
    """
    import pandas as pd
    def parse_date(date_str):
        try:
            return pd.to_datetime(date_str, format='%d/%m/%Y').strftime("%Y-%m-%d")
        except Exception:
            return None
    assert parse_date(date_str) == expected